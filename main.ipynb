#%%

import numpy as np
from torch_linear_svm import SGD_SVM_Torch, argp
from non_iid_split import split_img_noniid
import torch
from sklearn.metrics import accuracy_score
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

#%%

def initialization(x,y,n_user = 50):
    model_set = dict()
    x_n_iid, y_n_iid = split_img_noniid(x, y)
    for i in range(n_user):
        model_set[i] = SGD_SVM_Torch(args=argp(epoch=1),lr_mode='Optimal')

    return model_set, x_n_iid,y_n_iid

data_preprocessed = pd.read_csv('/home/ericlong/PycharmProjects/pythonProject'
                                '/TKDE_data_figure/extracted_feature_table.csv')
x_train, y_train = np.array(data_preprocessed.iloc[:, 1:513]), np.array(data_preprocessed.iloc[:, 513])
PCA_decompose = PCA(n_components=100)


x_train_decompose = PCA_decompose.fit_transform(x_train)

MOD, x_non_iid, y_non_iid = initialization(x=x_train_decompose,y= y_train)
data_union = [x_non_iid,y_non_iid]
print('Datatype of x_non_iid',type(x_non_iid),len(x_non_iid))
print('Model type used:', MOD[0])

#%%

def client_update_collect(models:dict, distributed_data, weights_obtain=None, bias_obtain=None, epoch_index=None):
    # models contain all the model of clients, we conduct federated learning in one device. Every
    # model is class SGD_SVM_Torch
    # distributed_data stores (x,y) where x and y are shown in a decentralized manner.
    # weights_obtain denote the received weights from the central server after average
    n_user = len(models)
    if (type(weights_obtain) == type(None)) and (type(bias_obtain) == type(None)):
        print('Start from initialization')
        for i in range(n_user):
            # take string on i due to that we store the data in dictionary with key i.
            models[i].fit(x=distributed_data[0][str(i)],y=distributed_data[1][str(i)])
    else:
        print('Epoch update:', epoch_index)
        for i in range(n_user):
            models[i].learning_rate = 0.0001/epoch_index
            # if we take 'Optimal Rate' mode, a small change in epoch would not impact the result so much
            # so force the reduction of learning rate. Assume that 0.0001 is the lambda we set.
            models[i].fit(x=distributed_data[0][str(i)], y=distributed_data[1][str(i)], weights=weights_obtain,
                          bias = bias_obtain)
    return models

models_epoch1 = client_update_collect(models=MOD, distributed_data=data_union)

#%%

def evaluate(models, distributed_data):
    x_clients = distributed_data[0]
    y_clients = distributed_data[1]

    acc = list()

    for i in range(len(models)):
        pred = models[i].predict(x_new=x_clients[str(i)])
        acc.append(accuracy_score(y_true=y_clients[str(i)],y_pred=pred))

    return acc

acc_temp1 = evaluate(models=models_epoch1,distributed_data=data_union)
print('The classification of accuracy for the first epoch, by users:', acc_temp1)

#%%

def model_average(models, distributed_data):
    # the slice index 1 stands for y.
    n_client = [len(i) for i in distributed_data[1]]
    n = sum(n_client)
    weights = torch.zeros(size=models[0].weights.size())
    bias = torch.zeros(size=models[0].bias.size())

    for i in range(len(models)):
        weights += (n_client[i]/n)*models[i].weights
        bias += (n_client[i]/n)*models[i].bias

    return  weights, bias

weights, bias = model_average(models=models_epoch1, distributed_data=data_union)
models_epoch2 = client_update_collect(models=models_epoch1, distributed_data=data_union, weights_obtain=weights,
                                      bias_obtain=bias, epoch_index=2)
acc_temp2 = evaluate(models=models_epoch2,distributed_data=data_union)

#%%

diff_1_2 = np.array(acc_temp2)-np.array(acc_temp1)
plt.plot(range(50),diff_1_2, 'go')
plt.axhline(y=0, color='r', linestyle='--')
#plt.plot(range(50), acc_temp1, 'b+')
#plt.plot(range(50), acc_temp2, 'r+')
plt.title('Result comparison between the first and second epoch')
plt.xlabel('User Index')
plt.ylabel('Classification Accuracy')
plt.show()
print('Mean of Accuracy changed from Epoch 1 to 2:', np.mean(diff_1_2))

#%%

clf = SGD_SVM_Torch(argp(epoch=5),lr_mode='Optimal')
clf.fit(x=x_train_decompose, y=y_train)
model_full = dict()
for i in range(50):
    model_full[i] = clf
acc_full = evaluate(models= model_full, distributed_data = data_union)

diff_1_central_train = np.array(acc_full)-np.array(acc_temp1)
plt.plot(range(50),diff_1_central_train, 'go')
plt.axhline(y=0, color='r', linestyle='--')
#plt.plot(range(50), acc_temp1, 'b+')
#plt.plot(range(50), acc_temp2, 'r+')
plt.title('Result comparison between the first epoch and centralized training model')
plt.xlabel('User Index')
plt.ylabel('Classification Accuracy')
#plt.savefig('centralized_training.jpg')
plt.show()
print('Mean of Accuracy changed from Epoch 1 to full central train:', np.mean(diff_1_central_train))

#%%

total_epoch = 100

# Initialization

model_former = MOD
weights = None
bias = None
data_use = data_union

for ep in range(1, total_epoch):
    model_latter = client_update_collect(models=model_former, distributed_data=data_use,
                                         weights_obtain=weights, bias_obtain=bias,epoch_index=1)
    acc = evaluate(model_latter,distributed_data=data_use)
    print(np.mean(acc))
    weights, bias = model_average(model_latter, distributed_data=data_use)
    model_former = client_update_collect(model_latter, distributed_data=data_use,
                                         weights_obtain=weights, bias_obtain=bias, epoch_index= 1)

diff = np.array(acc)-np.array(acc_temp1)





#%%

plt.plot(range(50),diff, 'go')
plt.axhline(y=0, color='r', linestyle='--')
#plt.plot(range(50), acc_temp1, 'b+')
#plt.plot(range(50), acc_temp2, 'r+')
plt.title('Result comparison between the first epoch and decentralized training model')
plt.xlabel('User Index')
plt.ylabel('Classification Accuracy')
#plt.savefig('decentralized_training_CM100.jpg')
plt.show()
print('Mean of accuracy increased:', np.mean(diff))
